{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#installing the necessary modules to extract data from API\n",
    "pip install investpy\n",
    "pip install yfinance\n",
    "pip install pandas_datareader\n",
    "pip install pandas_ta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary modules\n",
    "from pandas_datareader import data as pdr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import investpy\n",
    "import yfinance as yf\n",
    "yf.pdr_override() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get indices from Yahoo Finance API. We will be extracting daily frequency data from 01/01/2010 to 31/12/2020\n",
    "index_data = yf.download(\"^GSPC ^DJI ^FTSE ^GDAXI FTSEMIB.MI ^FCHI ^IBEX ^AXJO ^HSI ^N225 ^SSMI ^ISEQ ^OMX ^AEX ^ATX \\\n",
    "       ^BFX  ^BUX 000001.SS ^BSESN ^KS11 ^GSPTSE\", start=\"2010-01-01\", end=\"2020-12-31\")\n",
    "\n",
    "#we will only extract the daily closing price of indices\n",
    "fundamental_indicator = index_data['Close'].fillna(np.nan)\n",
    "print(fundamental_indicator.head())\n",
    "\n",
    "#save the extracted data to csv file\n",
    "fundamental_indicator.to_csv('fundamental_indicator_yahoo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to rename the columns in fundamental_indicators to, for example China index, will be ShanghaiSE-china\n",
    "index_list = ['ShanghaiSE', 'ftse mib', 'aex', 'atx', 'asx200', 'bfx', 'bsesn', 'bux', 'djia','cac40', 'ftse100', 'dax30', 's&p500', 'S&P/TSX', 'hang seng', 'ibex 35', 'iseq', 'kospi', 'nikkei225', \\\n",
    "              'omx', '^SSMI']\n",
    "country_list = ['china', 'italy', 'netherlands', 'austria', 'australia', 'belgium', 'india', 'hungary', 'united states', 'france', 'united kingdom', 'germany', 'united states', 'canada', 'hong kong', \\\n",
    "                'spain', 'ireland', 'south korea', 'japan', 'sweden', 'switzerland' ]\n",
    "\n",
    "column_names = fundamental_indicator.columns\n",
    "#this renames the column names\n",
    "for h,i,j in zip(column_names, index_list, country_list):\n",
    "    fundamental_indicator = fundamental_indicator.rename(columns= { h : i + '_' + j })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since I am unable to obtain Russia, Poland, Denmark and Finland indices from Yahoo Finance API, I will\n",
    "#obtain them from Investing.com API(investpy)\n",
    "\n",
    "idx = pd.date_range('2010-01-01', '2020-12-31')\n",
    "index_list_3 = ['moex', 'wig20', 'omxc20', 'omx helsinki 25' ]\n",
    "country_list_3 = ['russia', 'poland', 'denmark', 'finland']\n",
    "for i,j in zip(index_list_3, country_list_3):\n",
    "    df = investpy.get_index_historical_data(index= i,\n",
    "                                        country= j,\n",
    "                                        from_date='01/01/2010',\n",
    "                                        to_date='31/12/2020')\n",
    "    df_retained = df['Close']\n",
    "\n",
    "    #due to indices in Investing.com API not having NaN values for certain days due to public holiday, we need to insert rows with\n",
    "    #with NaN values in those days\n",
    "    df_retained.index = pd.DatetimeIndex(df_retained.index)\n",
    "    df_retained_1 = df_retained.reindex(idx, fill_value=np.nan)\n",
    "\n",
    "    #this adds the new column to the fundamental_indicator dataframe\n",
    "    fundamental_indicator[j + '-' + i] = df_retained_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we extract exchange rate data from Yahoo Finance API\n",
    "#Since we are predicting closing price of Switzerland stock index, we will pair CHF against another currency\n",
    "\n",
    "currency_data = yf.download(\"NOKCHF=X CHFUSD=X CHFEUR=X CHFAUD=X CHFHKD=X JPYCHF=X CHFDKK=X CHFSEK=X CHFNOK=X CHFPLN=X CHFCZK=X CHFHUF=X, \\\n",
    "                   CHFCNY=X CHFINR=X CHFKRW=X CHFCAD=X\", start=\"2010-01-01\", end=\"2020-12-31\")\n",
    "\n",
    "exchange_rate = currency_data['Close'].fillna(np.nan)\n",
    "exchange_rate.to_csv('C:/Users/user/Desktop/for udacity stock prediction deep learning project/exchange_rate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#due to different public holidays in different countries, we need to reindex the indices, bond yield and\n",
    "#exchange rate data before concatenating them into a single dataframe\n",
    "\n",
    "idx = pd.date_range('2010-01-01', '2020-12-31')\n",
    "indicator_list = [fundamental_indicator, exchange_rate]\n",
    "for i in indicator_list:\n",
    "    i.index = pd.DatetimeIndex(i.index)\n",
    "    i = i.reindex(idx, fill_value=np.nan)\n",
    "\n",
    "#reindex and fill rows with no data with NaN\n",
    "fundamental_indicator_2 = fundamental_indicator.reindex(idx, fill_value=np.nan)\n",
    "exchange_rate_2 = exchange_rate.reindex(idx, fill_value=np.nan)\n",
    "print(fundamental_indicator_2)\n",
    "print(exchange_rate_2)\n",
    "\n",
    "#all the 3 dataset have 4018 rows after reindexing\n",
    "#concatenate the indices, bond yield and exchange rate data into a single dataframe\n",
    "all_indicators = pd.concat([fundamental_indicator_2, exchange_rate_2], axis = 1)\n",
    "print(all_indicators)\n",
    "\n",
    "#after concatenating, dataframe has 4018 rows and 62 columns.(25 for indices, 21 for exchange rate and\n",
    "#16 for exchange rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will be using pandas_ta module to extract technical indicator variables.\n",
    "#We will be using 3, 5, 7, 10, 15, 20, 25 and 30 days in the past for our technical indicators.\n",
    "\n",
    "import pandas_ta as ta\n",
    "length = [3, 5, 7, 10, 15, 20, 25, 30]\n",
    "#Since we will be forecasting Switzerland stock index, we need to extract Open,Close,High and Low daily price\n",
    "#which are used to calculate the technical indicators\n",
    "df_smi = investpy.get_index_historical_data(index= 'smi', country= 'switzerland', from_date='01/01/2010',\n",
    "                                            to_date='31/12/2020')\n",
    "df_smi_close = df_smi['Close']\n",
    "df_smi_high = df_smi['High']\n",
    "df_smi_low = df_smi['Low']\n",
    "df_smi_open = df_smi['Open']\n",
    "\n",
    "#extracting simple moving average\n",
    "sma_ta = pd.DataFrame()\n",
    "for i in length:\n",
    "    sma_ta_1 = ta.sma(df_smi_close, length = i)\n",
    "    sma_ta['sma_' + str(i) ] = sma_ta_1\n",
    "\n",
    "#extracting exponential moving average\n",
    "help(ta.ema)\n",
    "ema_ta = pd.DataFrame()\n",
    "for i in length:\n",
    "    ema_ta_1 = ta.ema(df_smi_close, length = i)\n",
    "    ema_ta['ema_' + str(i)] = ema_ta_1\n",
    "\n",
    "#extracting average true range\n",
    "help(ta.atr)\n",
    "atr_ta = pd.DataFrame()\n",
    "for i in length:\n",
    "    atr_ta_1 = ta.atr(df_smi_high, df_smi_low, df_smi_close, length = i)\n",
    "    ema_ta['atr_' + str(i)] = atr_ta_1\n",
    "\n",
    "#extracting relative strength index\n",
    "help(ta.rsi)\n",
    "rsi_ta = pd.DataFrame()\n",
    "for i in length:\n",
    "    rsi_ta_1 = ta.rsi(df_smi_close, length = i)\n",
    "    rsi_ta['rsi_' + str(i)] = rsi_ta_1\n",
    "\n",
    "#extracting average directional index\n",
    "help(ta.adx)\n",
    "adx_ta = pd.DataFrame()\n",
    "for i in length:\n",
    "    adx_ta_1 = ta.adx(df_smi_high, df_smi_low, df_smi_close, length = i)\n",
    "    #print(adx_ta_1)\n",
    "    adx_ta['adx_' + str(i)] = adx_ta_1.iloc[:,0]\n",
    "\n",
    "#extracting stochastic oscillators\n",
    "help(ta.stoch)\n",
    "stoch_k_ta = pd.DataFrame()\n",
    "stoch_d_ta = pd.DataFrame()\n",
    "for i in length:\n",
    "    stoch_ta_1 = ta.stoch(df_smi_high, df_smi_low, df_smi_close, k = i, d = 3, smooth_k = i)\n",
    "    stoch_k_ta['stoch_k_' + str(i)] = stoch_ta_1.iloc[:,0]\n",
    "    stoch_d_ta['stoch_d_' + str(i)] = stoch_ta_1.iloc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all technical indicator variables into a single dataframe\n",
    "technical_indicators = pd.concat([sma_ta, ema_ta, atr_ta, rsi_ta, adx_ta, stoch_k_ta, stoch_d_ta], axis = 1)\n",
    "technical_indicators.to_csv('C:/Users/user/Desktop/for udacity stock prediction deep learning project/technical_indicators_3.csv')\n",
    "\n",
    "#reindexing the concatenated dataframe\n",
    "technical_indicators_2 = technical_indicators.reindex(idx, fill_value=np.nan)\n",
    "#concatenate the previous fundamental indicators with technical indicators\n",
    "all_indicators_funda_tech = pd.concat([all_indicators, technical_indicators_2], axis = 1)\n",
    "\n",
    "#convert all variables data type to float\n",
    "all_indicators_funda_tech_2 = all_indicators_funda_tech.astype(float)\n",
    "#since first 3 rows of dataframe are all missing values, we exclude them\n",
    "all_indicators_funda_tech_2 = all_indicators_funda_tech_2.iloc[3:,:]\n",
    "#to fill missing values, we use linear interpolation since this is sequential data\n",
    "all_indicators_funda_tech_2 = all_indicators_funda_tech_2.interpolate().ffill().bfill()\n",
    "#since up to 84 rows for stochastic %D have missing values, causing linear interpolation to fill them with same values,\n",
    "#I have to remove the first 84 rows\n",
    "all_indicators_funda_tech_2 = all_indicators_funda_tech_2.iloc[84:-1,:]\n",
    "all_indicators_funda_tech_2.to_csv('C:/Users/user/Desktop/for udacity stock prediction deep learning project/all_indicators_fundamental_technical_2.csv')\n",
    "\n",
    "#to remove seasonality and trend from the data, i have to apply logarithm difference\n",
    "log_return_all_indicators = pd.DataFrame()\n",
    "column_names_1 = all_indicators_funda_tech_2.columns\n",
    "\n",
    "log_return_all_indicators = np.log(all_indicators_funda_tech_2).diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now extract Switzerland stock index closing price data since this is our target variable\n",
    "switzerland_index_data = yf.download(\"^SSMI\", start=\"2010-01-01\", end=\"2020-12-31\")\n",
    "switzerland_index_data_close = switzerland_index_data['Close']\n",
    "switzerland_index_data_close.to_csv('C:/Users/user/Desktop/for udacity stock prediction deep learning project/switzerland stock index historical data.csv')\n",
    "\n",
    "#We would include 1, 3 and 5 days for our forecast horizon\n",
    "switzerland_forecast_horizon_return = pd.DataFrame()\n",
    "#convert all values to natural log\n",
    "switzerland_index_data_close = np.log(switzerland_index_data_close)\n",
    "forecast_horizon = [1, 3, 5]\n",
    "\n",
    "#we need to do 1,3 and 5 days period differencing to get the returns\n",
    "for i in forecast_horizon:\n",
    "    switzerland_forecast_horizon_return['forecast horizon_' + str(i) + '_day'] = switzerland_index_data_close.diff(periods = i)\n",
    "\n",
    "k = 0\n",
    "for i in forecast_horizon:\n",
    "    switzerland_forecast_horizon_return.iloc[:,k] = switzerland_forecast_horizon_return.shift(-i)\n",
    "    k += 1\n",
    "\n",
    "#drop missing values\n",
    "switzerland_forecast_horizon_return = switzerland_forecast_horizon_return.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to merge the Switzerland stock index return with the independent features.\n",
    "all_indicators_with_forecast_return =pd.merge(log_return_all_indicators, switzerland_forecast_horizon_return, how=\"right\",\n",
    "    on=None, left_on=None, right_on=None, left_index=True, right_index=True, sort=True, suffixes=(\"_x\", \"_y\"), copy=True,\n",
    "    indicator=False, validate=None)\n",
    "all_indicators_with_forecast_return = all_indicators_with_forecast_return.dropna()\n",
    "forecast_return_classification = all_indicators_with_forecast_return.iloc[:,-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#we would need to split the dataset into 75% training data and 25% test data\n",
    "total_rows = all_indicators_with_forecast_return.shape[0]\n",
    "training_size = int(0.75 * total_rows)\n",
    "X_train = all_indicators_with_forecast_return.iloc[:training_size,:-3]\n",
    "X_test = all_indicators_with_forecast_return.iloc[training_size:,:-3]\n",
    "\n",
    "#we need to convert the returns for our target variables into binary values: 1 for upward movement\n",
    "#and 0 for downward movement\n",
    "forecast_return_classification_binary = ((forecast_return_classification.values) > 0).astype(int)\n",
    "\n",
    "#we then split the target variable into y_train and y_test\n",
    "y_train = forecast_return_classification_binary[:training_size,:]\n",
    "y_test = forecast_return_classification_binary[training_size:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To find the 10 most important features, we will use chi squared test and mutual info classification\n",
    "from sklearn.feature_selection import chi2, mutual_info_regression, mutual_info_classif\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "selected_features = SelectKBest(mutual_info_classif, k=10).fit(X_train, y_train[:,2])\n",
    "column_names = X_train.columns\n",
    "interesting_features_df = pd.DataFrame(selected_features.scores_, index=column_names).rename(columns={0:'feature weight'})\n",
    "\n",
    "#this sorts the features from highest weight to lowest weight\n",
    "interesting_features_df = interesting_features_df.sort_values('feature weight', ascending=False)\n",
    "\n",
    "#this displays the 10 most important features in horizontal bar chart and the corresponding weights\n",
    "import matplotlib.pyplot as plt\n",
    "Features = interesting_features_df.index[0:10].to_list()[::-1]\n",
    "Weight = interesting_features_df['feature weight'][0:10].to_list()[::-1]\n",
    "\n",
    "plt.barh(Features, Weight, color='orange')\n",
    "plt.title('10 MOST IMPORTANT FEATURES')\n",
    "plt.ylabel('FEATURES')\n",
    "plt.xlabel('WEIGHT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the chi squared test to find the 10 most important features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "selected_features = SelectKBest(chi2, k=10).fit(X_train_scaled, y_train[:,2])\n",
    "interesting_features_df = pd.DataFrame(selected_features.scores_, index=column_names).rename(columns={0:'feature weight'})\n",
    "\n",
    "#this sorts the features from highest weight to lowest weight\n",
    "interesting_features_df = interesting_features_df.sort_values('feature weight', ascending=False)\n",
    "\n",
    "#this displays the 10 most important features in horizontal bar chart and the corresponding weights\n",
    "import matplotlib.pyplot as plt\n",
    "Features = interesting_features_df.index[0:10].to_list()[::-1]\n",
    "Weight = interesting_features_df['feature weight'][0:10].to_list()[::-1]\n",
    "\n",
    "plt.barh(Features, Weight, color='orange')\n",
    "plt.title('10 MOST IMPORTANT FEATURES')\n",
    "plt.ylabel('FEATURES')\n",
    "plt.xlabel('WEIGHT')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary modules for bayesian optimization\n",
    "import ast\n",
    "import csv\n",
    "import inspect\n",
    "import sys\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import pickle\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = \"bayes_optimization_summary_{}.csv\".format(\n",
    "    strftime(\"%y%m%d%H%M\", gmtime())\n",
    ")\n",
    "\n",
    "#used to name the csv file for recording accuracy and F1 scores and hyperparameters combination\n",
    "with open(results_file, \"w\", newline=\"\") as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"iteration\",\n",
    "            \"loss\",\n",
    "            \"accuracy_score\",\n",
    "            \"f1_score\"\n",
    "            \"train_time\",\n",
    "            \"params\",\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(space):\n",
    "\n",
    "    global ITERATION\n",
    "    ITERATION += 1\n",
    "    start = timer()\n",
    "    global predictions_all\n",
    "\n",
    "    \n",
    "    #train the XGBClassifier model on training data and test data is used as validation data with early stopping round set at 40\n",
    "    eval_set = [(X_test, y_test[:,2])]\n",
    "    xgbcl = xgb.XGBClassifier(colsample_bytree =  space['colsample_bytree'], gamma = space['gamma'], learning_rate = space['learning_rate'], max_depth = space['max_depth'], min_child_weight = space['min_child_weight'], n_estimators = space['n_estimators'], reg_alpha = space['reg_alpha'], scale_pos_weight = space['scale_pos_weight'], subsample = space['subsample'], tree_method = 'gpu_hist', sampling_method = 'gradient_based')\n",
    "    xgbcl.fit(X_train, y_train[:,2], early_stopping_rounds=40, eval_metric=\"error@0.55\", eval_set=eval_set)\n",
    "    #the trained model is used to predict on the test data\n",
    "    prediction = xgbcl.predict(X_test)\n",
    "    predictions_all = pd.concat([predictions_all, pd.DataFrame(prediction).rename(columns={0:ITERATION})], axis=1)\n",
    "    #obtain the accuracy and f1 score\n",
    "    acc = accuracy_score(y_test[:,2], prediction)\n",
    "    f1 = f1_score(y_test[:,2], prediction) \n",
    "\n",
    "    # log runtime\n",
    "    run_time = timer() - start\n",
    "\n",
    "    # calculate loss based on scoring method\n",
    "    loss = 1 - acc\n",
    "    \n",
    "    # export results to csv\n",
    "    out_file = results_file\n",
    "    with open(out_file, \"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(\n",
    "            [\n",
    "                ITERATION,\n",
    "                loss,\n",
    "                acc,\n",
    "                f1,\n",
    "                run_time,\n",
    "                space\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the hyperparameter search space for XGBoost Classifier\n",
    "space = { 'learning_rate': hp.uniform('learning_rate', 0.001, 0.050),\n",
    "'max_depth': hp.choice('max_depth', np.arange(4, 14, 1, dtype=int)),\n",
    "'min_child_weight': hp.choice('min_child_weight', np.arange(1, 20, 1, dtype = int)),\n",
    "'gamma': hp.uniform('gamma', 0.04, 0.4),\n",
    "'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
    "'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
    "'reg_alpha': hp.uniform('reg_alpha', 0.002 , 0.04),\n",
    "'n_estimators': hp.choice('n_estimators', np.arange(200, 500,1, dtype = int)),\n",
    "'scale_pos_weight': hp.choice('scale_pos_weight', np.arange(10, 50, 1, dtype=int)),\n",
    "'tree_method' :'gpu_hist',\n",
    "'sampling_method' : 'gradient_based'\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this line of codes initiates the bayesian optimisation procedure and I will set maximum iterations at 400\n",
    "ITERATION=0\n",
    "best = fmin(\n",
    "    fn=objective,\n",
    "    space=space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=400,\n",
    "    trials=Trials(),\n",
    "    show_progressbar=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
